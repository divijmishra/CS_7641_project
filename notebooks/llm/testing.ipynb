{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"../../data/\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from llm.llm_utils import (\n",
    "    split_and_save_data,\n",
    "    load_data_as_lists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data splits\n",
    "# raw_data_path = data_path + \"raw/generator=base~dataset=p1.00.jsonl\"\n",
    "# split_and_save_data(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Patrons crowd the platform at the Washington Metropolitan Area Transit Authority's (WMATA) Metro Center stop in Washington, D.C. on Dec. 20, 2004. Thousands use the public transit system daily to get them in and around the D.C. area. (Karen Bleier/AFP/Getty Images)\\nUS Senators Threaten Metro Funding Over Chinese Manufacturer\\nWASHINGTON —Federal lawmakers say they’ll approve badly needed funding for Washington’s transit system, but only if it avoids buying new rail cars from China.\\nThe Washington Post reported on April 13 that U.S. Senators from Virginia and Maryland proposed the idea in new legislation. It reflects growing concerns that China’s state-owned rail company could hurt American manufacturers and make the system vulnerable to cyber espionage.\\nDave Smolensky, spokesman for the China Railway Rolling Stock Corp, dismissed the espionage concerns. The company also said the United States should be promoting competition.\\nThe company has won four major U.S. rail car contracts. It is pursuing a Metro contract worth more than $1 billion to build up to 800 of the new rail cars.\\nThere are no U.S. transit rail car manufacturers. The bidding deadline is May 31.\\n\", 'On April 12, I wrote \"With lows of nearly $45 a barrel, historically undisturbed deep discounting,\" as to why companies were trading off a window of opportunity with the lowest valuation they had seen in years.\\nHigh valuations\\nI am not an expert, but I would like to illustrate that many valuation metrics are insufficient to evaluate oil\\'s current $45+ per barrel valuation.\\nThe supply story is probably over-hanging at first, as it appears the black hole in cap market equity is only around $70 #2. This means if we were to seek weekly inventory data, that number would spiral downward to $70 #4. This is gradual, and if this $60 to $70 levels is not expected to be sustained as a matter of course we would be under replacement. As of January, the supply shortage is at least $990 #526.\\nInventory data history\\nAs of late exhibit, the supply at year-ends fell from above 718 mn taken down by 60-70 such cases in 1900. I believe this is obviously overdone. It has nothing to do with current oil prices, if anything, but how at the right price to the right price a supply deficit.\\nLab Turns Out (Spectrant Worldwide, ELITE, DCArch, Photovia) 1971 Oil tank filled with wax from old tankers is loaded on the New York Mercantile Exchange. All was well and he was reassessed. Alamed being on a cigar\\nLooking at the graph, work lease 2017 numbers means that 3/4 at 10-11 a.m. are refurbished to current price. When oil tries to read $60+ the strain is hard, and thus this is the season for purchases of petroleum products. In that cycle of about 3 days, 44 reaccommodation of oil, then 22 new refineries, equals closer to 55 per day. For real time, this means that the motivation for refineries upgrading is to replace raw petroleum product.\\nTaking the Pieces/Tracking Supply Base\\nNote that is NOT a \"bubble theory\", it\\'s simply an attempt to compare how fast oil prices have fallen. Among the 5 biggest OPEC producer-makers were Russia, Saudi Arabia, and Venezuela. They vastly outnumbered those other 9 (and this is why most of them are not in debt). The stock market has gone from bubbling up about $150 a barrel to hovering around $80 a barrel. The same seems to be happening about $60-$85/bbl. Realistically this increases the value of the company.\\nPushing Production\\nStill the bull\\'s eye. We have to accept that temperature increase is not an option, and clearly the OPEC oil cartel is not willing to \"survive\" the current efforts by several producers to reduce their inventories. This will get you nowhere; the resulting glut (outdated and experiencing a three-year decline in production) will significantly reduce all supplies this summer (particularly outside North America). The true end is yet to come.\\nOiltanking the Skynet\\nOil is also going down, and so will the housing, energy technology (based on the availability of drilling technology in the gulf and offshore), and some more pump-up at the macro level. While this might tug the cloud out of the picture, I do not see how the bubble will stop.\\nThe medical benefits, food, and healthcare for operators have changed in recent years, and the medical benefits of equipment that attaches to a small minority of aging, diseased, or sick people already matter. Absence of these kinds of discrete, cost-saving medical biotechnologies should open up \"revenue pool and all...collateralize (er) by power activation and deployment of multiple ‘missile…’ bundles.\" Currently, the shelf space looks well covered by many insurers, major provider of leg surgery and ophthalmic treatment, telemedicine, and emergency room outages in the U.S. So there could be as many as 15 billion SSR contracts rolled off and thus the commercial options could morph into that \"robust ETNs that SNL keeps his name in.\" As it relates to PRO investing we\\'re working on this. A good few companies are near exits and many REITs and large corporate issuers are passing their funds under the bus.\\nAnd if costs can be removed and the companies that have succeeded can increase again higher dividend yields for holders of higher-yielding options, here is how to play the downturn: If you rate your chances of dividend increases poorly, sell or buy anything in agriculture - crank up the dividend yield if it is low, and add sugar, cotton or corn (assuming it will continue to rise in price) if things in oil decline.\\nDisclosure: I am/we are long VSTL.\\nI wrote this article']\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_data = load_data_as_lists(data_path + \"splits/train.jsonl\")\n",
    "val_data = load_data_as_lists(data_path + \"splits/val.jsonl\")\n",
    "test_data = load_data_as_lists(data_path + \"splits/test.jsonl\")\n",
    "\n",
    "print(train_data[0][:2])\n",
    "print(train_data[1][:2])  # 0 corresponds to human, 1 corresponds to machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train samples: 10000\n",
      "Num of test samples: 12000\n",
      "Num of val samples: 3000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num of train samples: {len(train_data[0])}\")\n",
    "print(f\"Num of test samples: {len(test_data[0])}\")\n",
    "print(f\"Num of val samples: {len(val_data[0])}\")\n",
    "# why are there 12k test samples wtf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataset functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": data_path + \"splits/train.jsonl\",\n",
    "    \"val\" : data_path + \"splits/val.jsonl\",\n",
    "    \"test\": data_path + \"splits/test.jsonl\",\n",
    "}\n",
    "grover_dataset = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': \"Patrons crowd the platform at the Washington Metropolitan Area Transit Authority's (WMATA) Metro Center stop in Washington, D.C. on Dec. 20, 2004. Thousands use the public transit system daily to get them in and around the D.C. area. (Karen Bleier/AFP/Getty Images)\\nUS Senators Threaten Metro Funding Over Chinese Manufacturer\\nWASHINGTON —Federal lawmakers say they’ll approve badly needed funding for Washington’s transit system, but only if it avoids buying new rail cars from China.\\nThe Washington Post reported on April 13 that U.S. Senators from Virginia and Maryland proposed the idea in new legislation. It reflects growing concerns that China’s state-owned rail company could hurt American manufacturers and make the system vulnerable to cyber espionage.\\nDave Smolensky, spokesman for the China Railway Rolling Stock Corp, dismissed the espionage concerns. The company also said the United States should be promoting competition.\\nThe company has won four major U.S. rail car contracts. It is pursuing a Metro contract worth more than $1 billion to build up to 800 of the new rail cars.\\nThere are no U.S. transit rail car manufacturers. The bidding deadline is May 31.\\n\",\n",
       " 'domain': 'theepochtimes.com',\n",
       " 'title': 'US Senators Threaten Metro Funding Over Chinese Manufacturer',\n",
       " 'date': 'April 15, 2019',\n",
       " 'authors': None,\n",
       " 'ind30k': 29363,\n",
       " 'url': 'https://www.theepochtimes.com/us-senators-threaten-metro-funding-over-chinese-manufacturer_2880764.html',\n",
       " 'label': 'human',\n",
       " 'orig_split': 'train_burner',\n",
       " 'split': 'train',\n",
       " 'random_score': -4.26355665680627,\n",
       " 'top_p': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grover_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article length analysis\n",
    "lens = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "for split in grover_dataset.keys():\n",
    "    for i in range(len(grover_dataset[split])):\n",
    "        text = grover_dataset[split][i]['article']\n",
    "        lens[split].append(len(text.split()))\n",
    "\n",
    "print(sorted(lens[\"train\"], reverse=True)[:10])\n",
    "# plt.hist(lens['train'], bins=50)\n",
    "print(sorted(lens[\"val\"], reverse=True)[:10])\n",
    "# plt.hist(lens['val'], bins=50)\n",
    "print(sorted(lens[\"test\"], reverse=True)[:10])\n",
    "# plt.hist(lens['test'], bins=50)\n",
    "\n",
    "#### The longest article seems to be 13k words!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': label\n",
      "human      5000\n",
      "machine    5000\n",
      "Name: count, dtype: int64, 'val': label\n",
      "human      2000\n",
      "machine    1000\n",
      "Name: count, dtype: int64, 'test': label\n",
      "human      8000\n",
      "machine    4000\n",
      "Name: count, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# data imbalance checking\n",
    "label_counts = {}\n",
    "for split in grover_dataset.keys():\n",
    "    df = pd.DataFrame(grover_dataset[split])\n",
    "    label_counts[split] = df['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# weird, train is balanced, but val and test are imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the model (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import bitsandbytes as bnb\n",
    "import accelerate\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '../../models/hf_cache/'\n",
    "\n",
    "from transformers.utils import is_accelerate_available, is_bitsandbytes_available\n",
    "print(is_accelerate_available())\n",
    "print(is_bitsandbytes_available())\n",
    "print(torch.cuda.is_available())\n",
    "# is_bitsandbytes_available() only returns True if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/storage/ice1/4/1/dmishra45/CS-7641-Project/notebooks/llm'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85e513f2c994608850dc87186bab434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8439f50cb73348a0b2b2611453575ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce827789752446ef9251a12e473027c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b2361f34884d8f8d8123d5b6047363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# load the model\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     cache_dir = os.environ['HF_HOME'] + \"hub/\",\n",
    "# )\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir = os.environ['HF_HOME'] + \"hub/\",\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what you get when you print the model \n",
    "# after successfully loading it with bnb\n",
    "\"\"\"\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(\n",
    "    (embed_tokens): Embedding(32000, 4096)\n",
    "    (layers): ModuleList(\n",
    "      (0-31): 32 x LlamaDecoderLayer(\n",
    "        (self_attn): LlamaSdpaAttention(\n",
    "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (rotary_emb): LlamaRotaryEmbedding()\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm()\n",
    "        (post_attention_layernorm): LlamaRMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): LlamaRMSNorm()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '../../models/hf_cache/'\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from llm.llm_utils import (\n",
    "    create_prompt,\n",
    "    tokenize_text\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "data_path = \"../../data/\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "MAX_TOKENS = 4096\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": data_path + \"splits/train.jsonl\",\n",
    "    \"val\" : data_path + \"splits/val.jsonl\",\n",
    "    \"test\": data_path + \"splits/test.jsonl\",\n",
    "}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir = os.environ['HF_HOME'] + \"hub/\",\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'domain', 'title', 'date', 'authors', 'ind30k', 'url', 'label', 'orig_split', 'split', 'random_score', 'top_p'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['article', 'domain', 'title', 'date', 'authors', 'ind30k', 'url', 'label', 'orig_split', 'split', 'random_score', 'top_p'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'domain', 'title', 'date', 'authors', 'ind30k', 'url', 'label', 'orig_split', 'split', 'random_score', 'top_p'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].map(create_prompt)\n",
    "# doesn't take long, ~15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instructions:\n",
      "Your task is to classify an excerpt from a news article as being human-generated or machine-generated. If it was machine-generated, respond with 'machine', else respond with 'human'.\n",
      "\n",
      "### Input:\n",
      "Classify the following news article excerpt as being human-generated or machine-generated:\n",
      "\n",
      "On April 12, I wrote \"With lows of nearly $45 a barrel, historically undisturbed deep discounting,\" as to why companies were trading off a window of opportunity with the lowest valuation they had seen in years.\n",
      "High valuations\n",
      "I am not an expert, but I would like to illustrate that many valuation metrics are insufficient to evaluate oil's current $45+ per barrel valuation.\n",
      "The supply story is probably over-hanging at first, as it appears the black hole in cap market equity is only around $70 #2. This means if we were to seek weekly inventory data, that number would spiral downward to $70 #4. This is gradual, and if this $60 to $70 levels is not expected to be sustained as a matter of course we would be under replacement. As of January, the supply shortage is at least $990 #526.\n",
      "Inventory data history\n",
      "As of late exhibit, the supply at year-ends fell from above 718 mn taken down by 60-70 such cases in 1900. I believe this is obviously overdone. It has nothing to do with current oil prices, if anything, but how at the right price to the right price a supply deficit.\n",
      "Lab Turns Out (Spectrant Worldwide, ELITE, DCArch, Photovia) 1971 Oil tank filled with wax from old tankers is loaded on the New York Mercantile Exchange. All was well and he was reassessed. Alamed being on a cigar\n",
      "Looking at the graph, work lease 2017 numbers means that 3/4 at 10-11 a.m. are refurbished to current price. When oil tries to read $60+ the strain is hard, and thus this is the season for purchases of petroleum products. In that cycle of about 3 days, 44 reaccommodation of oil, then 22 new refineries, equals closer to 55 per day. For real time, this means that the motivation for refineries upgrading is to replace raw petroleum product.\n",
      "Taking the Pieces/Tracking Supply Base\n",
      "Note that is NOT a \"bubble theory\", it's simply an attempt to compare how fast oil prices have fallen. Among the 5 biggest OPEC producer-makers were Russia, Saudi Arabia, and Venezuela. They vastly outnumbered those other 9 (and this is why most of them are not in debt). The stock market has gone from bubbling up about $150 a barrel to hovering around $80 a barrel. The same seems to be happening about $60-$85/bbl. Realistically this increases the value of the company.\n",
      "Pushing Production\n",
      "Still the bull's eye. We have to accept that temperature increase is not an option, and clearly the OPEC oil cartel is not willing to \"survive\" the current efforts by several producers to reduce their inventories. This will get you nowhere; the resulting glut (outdated and experiencing a three-year decline in production) will significantly reduce all supplies this summer (particularly outside North America). The true end is yet to come.\n",
      "Oiltanking the Skynet\n",
      "Oil is also going down, and so will the housing, energy technology (based on the availability of drilling technology in the gulf and offshore), and some more pump-up at the macro level. While this might tug the cloud out of the picture, I do not see how the bubble will stop.\n",
      "The medical benefits, food, and healthcare for operators have changed in recent years, and the medical benefits of equipment that attaches to a small minority of aging, diseased, or sick people already matter. Absence of these kinds of discrete, cost-saving medical biotechnologies should open up \"revenue pool and all...collateralize (er) by power activation and deployment of multiple ‘missile…’ bundles.\" Currently, the shelf space looks well covered by many insurers, major provider of leg surgery and ophthalmic treatment, telemedicine, and emergency room outages in the U.S. So there could be as many as 15 billion SSR contracts rolled off and thus the commercial options could morph into that \"robust ETNs that SNL keeps his name in.\" As it relates to PRO investing we're working on this. A good few companies are near exits and many REITs and large corporate issuers are passing their funds under the bus.\n",
      "And if costs can be removed and the companies that have succeeded can increase again higher dividend yields for holders of higher-yielding options, here is how to play the downturn: If you rate your chances of dividend increases poorly, sell or buy anything in agriculture - crank up the dividend yield if it is low, and add sugar, cotton or corn (assuming it will continue to rise in price) if things in oil decline.\n",
      "Disclosure: I am/we are long VSTL.\n",
      "I wrote this article.\n",
      "\n",
      "### Response:\n",
      "machine\n",
      "### End\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].remove_columns([\n",
    "    'article', 'domain', 'title', 'date', 'authors', 'ind30k', 'url', 'orig_split', 'split', 'random_score', 'top_p'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens when we tokenize stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 835, 2799, 582, 1953, 29901, 13, 10858, 3414, 338, 304, 770, 1598, 385, 429, 2265, 415, 515, 263, 9763, 4274, 408, 1641, 5199, 29899, 13525, 470, 4933, 29899, 13525, 29889, 960, 372, 471, 4933, 29899, 13525, 29892, 10049, 411, 525, 23523, 742, 1683, 10049, 411, 525, 26029, 4286, 13, 13, 2277, 29937, 10567, 29901, 13, 2385, 1598, 278, 1494, 9763, 4274, 429, 2265, 415, 408, 1641, 5199, 29899, 13525, 470, 4933, 29899, 13525, 29901, 13, 13, 2951, 3786, 29871, 29896, 29906, 29892, 306, 5456, 376, 3047, 301, 1242, 310, 8886, 395, 29946, 29945, 263, 2594, 2674, 29892, 3603, 1711, 563, 391, 28179, 6483, 2313, 792, 292, 1699, 408, 304, 2020, 14582, 892, 3534, 292, 1283, 263, 3474, 310, 15130, 411, 278, 19604, 17134, 362, 896, 750, 3595, 297, 2440, 29889, 13, 16382, 17134, 800, 13, 29902, 626, 451, 385, 17924, 29892, 541, 306, 723, 763, 304, 28475, 393, 1784, 17134, 362, 21556, 526, 1663, 29884, 4543, 304, 14707, 17182, 29915, 29879, 1857, 395, 29946, 29945, 29974, 639, 2594, 2674, 17134, 362, 29889, 13, 1576, 11421, 5828, 338, 3117, 975, 29899, 29882, 9776, 472, 937, 29892, 408, 372, 5692, 278, 4628, 16188, 297, 2117, 9999, 1592, 537, 338, 871, 2820, 395, 29955, 29900, 396, 29906, 29889, 910, 2794, 565, 591, 892, 304, 16508, 4723, 368, 11817, 706, 848, 29892, 393, 1353, 723, 6337, 284, 1623, 1328, 304, 395, 29955, 29900, 396, 29946, 29889, 910, 338, 4656, 950, 29892, 322, 565, 445, 395, 29953, 29900, 304, 395, 29955, 29900, 11174, 338, 451, 3806, 304, 367, 15075, 7114, 408, 263, 4383, 310, 3236, 591, 723, 367, 1090, 16920, 29889, 1094, 310, 5490, 29892, 278, 11421, 3273, 482, 338, 472, 3203, 395, 29929, 29929, 29900, 396, 29945, 29906, 29953, 29889, 13, 797, 23886, 848, 4955, 13, 2887, 310, 5683, 10371, 277, 29892, 278, 11421, 472, 1629, 29899, 1975, 8379, 515, 2038, 29871, 29955, 29896, 29947, 28597, 4586, 1623, 491, 29871, 29953, 29900, 29899, 29955, 29900, 1316, 4251, 297, 29871, 29896, 29929, 29900, 29900, 29889, 306, 4658, 445, 338, 12879, 975, 15091, 29889, 739, 756, 3078, 304, 437, 411, 1857, 17182, 26094, 29892, 565, 3099, 29892, 541, 920, 472, 278, 1492, 8666, 304, 278, 1492, 8666, 263, 11421, 822, 293, 277, 29889, 13, 28632, 9603, 29879, 4451, 313, 29903, 1103, 21867, 2787, 8157, 29892, 14845, 9094, 29892, 13681, 13197, 29892, 19040, 586, 423, 29897, 29871, 29896, 29929, 29955, 29896, 438, 309, 23735, 10423, 411, 281, 1165, 515, 2030, 23735, 414, 338, 7500, 373, 278, 1570, 3088, 4702, 29883, 424, 488, 24004, 29889, 2178, 471, 1532, 322, 540, 471, 337, 465, 11517, 29889, 838, 2795, 1641, 373, 263, 29507, 279, 13, 14959, 292, 472, 278, 3983, 29892, 664, 454, 559, 29871, 29906, 29900, 29896, 29955, 3694, 2794, 393, 29871, 29941, 29914, 29946, 472, 29871, 29896, 29900, 29899, 29896, 29896, 263, 29889, 29885, 29889, 526, 2143, 9265, 3276, 304, 1857, 8666, 29889, 1932, 17182, 14335, 304, 1303, 395, 29953, 29900, 29974, 278, 5312, 262, 338, 2898, 29892, 322, 4550, 445, 338, 278, 4259, 363, 10596, 2129, 310, 5697, 12154, 398, 9316, 29889, 512, 393, 11412, 310, 1048, 29871, 29941, 3841, 29892, 29871, 29946, 29946, 337, 562, 510, 1545, 362, 310, 17182, 29892, 769, 29871, 29906, 29906, 716, 2143, 4983, 583, 29892, 15743, 17649, 304, 29871, 29945, 29945, 639, 2462, 29889, 1152, 1855, 931, 29892, 445, 2794, 393, 278, 17385, 362, 363, 2143, 4983, 583, 20337, 292, 338, 304, 5191, 10650, 5697, 12154, 398, 3234, 29889, 13, 29911, 5086, 278, 26005, 778, 29914, 17936, 292, 9179, 368, 7399, 13, 9842, 393, 338, 6058, 263, 376, 29890, 23232, 6368, 613, 372, 29915, 29879, 3763, 385, 4218, 304, 7252, 920, 5172, 17182, 26094, 505, 19225, 29889, 17302, 278, 29871, 29945, 24842, 438, 4162, 29907, 14297, 29899, 29885, 21079, 892, 12710, 29892, 5701, 4749, 10387, 423, 29892, 322, 20931, 29889, 2688, 13426, 368, 714, 4537, 287, 1906, 916, 29871, 29929, 313, 392, 445, 338, 2020, 1556, 310, 963, 526, 451, 297, 2553, 29873, 467, 450, 10961, 9999, 756, 7695, 515, 289, 431, 21435, 701, 1048, 395, 29896, 29945, 29900, 263, 2594, 2674, 304, 16758, 292, 2820, 395, 29947, 29900, 263, 2594, 2674, 29889, 450, 1021, 2444, 304, 367, 10464, 1048, 395, 29953, 29900, 18039, 29947, 29945, 29914, 1327, 29880, 29889, 8195, 391, 1711, 445, 16415, 278, 995, 310, 278, 5001, 29889, 13, 29925, 21616, 19561, 13, 855, 453, 278, 289, 913, 29915, 29879, 10977, 29889, 1334, 505, 304, 3544, 393, 10430, 7910, 338, 451, 385, 2984, 29892, 322, 9436, 278, 438, 4162, 29907, 17182, 7774, 295, 338, 451, 17762, 304, 376, 7610, 29894, 573, 29908, 278, 1857, 14231, 491, 3196, 1391, 22543, 304, 10032, 1009, 11817, 3842, 29889, 910, 674, 679, 366, 1286, 4150, 29936, 278, 9819, 3144, 329, 313, 449, 9715, 322, 10623, 3277, 263, 2211, 29899, 6360, 4845, 457, 297, 5802, 29897, 674, 16951, 10032, 599, 28075, 445, 11801, 313, 1595, 16311, 368, 5377, 4644, 6813, 467, 450, 1565, 1095, 338, 3447, 304, 2041, 29889, 13, 29949, 2782, 804, 292, 278, 4971, 948, 300, 13, 29949, 309, 338, 884, 2675, 1623, 29892, 322, 577, 674, 278, 27261, 29892, 5864, 15483, 313, 6707, 373, 278, 20847, 3097, 310, 4192, 8873, 15483, 297, 278, 330, 16302, 322, 1283, 845, 487, 511, 322, 777, 901, 282, 3427, 29899, 786, 472, 278, 11758, 3233, 29889, 5806, 445, 1795, 260, 688, 278, 9570, 714, 310, 278, 7623, 29892, 306, 437, 451, 1074, 920, 278, 289, 23232, 674, 5040, 29889, 13, 1576, 16083, 23633, 29892, 9687, 29892, 322, 9045, 18020, 363, 12768, 505, 3939, 297, 7786, 2440, 29892, 322, 278, 16083, 23633, 310, 21083, 393, 10641, 267, 304, 263, 2319, 9461, 537, 310, 946, 292, 29892, 10267, 1463, 29892, 470, 17319, 2305, 2307, 4383, 29889, 24650, 663, 310, 1438, 17690, 310, 19554, 29892, 3438, 29899, 29879, 5555, 16083, 4768, 866, 3049, 11763, 881, 1722, 701, 376, 276, 9947, 11565, 322, 599, 856, 22017, 1008, 284, 675, 313, 261, 29897, 491, 3081, 26229, 322, 18209, 310, 2999, 5129, 9894, 488, 30098, 30010, 22813, 793, 1213, 15447, 29892, 278, 528, 761, 2913, 3430, 1532, 10664, 491, 1784, 1663, 332, 414, 29892, 4655, 13113, 310, 2814, 25300, 708, 322, 288, 561, 386, 284, 13076, 14502, 29892, 4382, 2168, 293, 457, 29892, 322, 11176, 14703, 5716, 714, 1179, 297, 278, 501, 29889, 29903, 29889, 1105, 727, 1033, 367, 408, 1784, 408, 29871, 29896, 29945, 24464, 5886, 29934, 8078, 29879, 29081, 1283, 322, 4550, 278, 12128, 3987, 1033, 18131, 964, 393, 376, 13716, 504, 382, 29911, 29940, 29879, 393, 21989, 29931, 14874, 670, 1024, 297, 1213, 1094, 372, 1104, 1078, 304, 13756, 13258, 292, 591, 29915, 276, 1985, 373, 445, 29889, 319, 1781, 2846, 14582, 526, 2978, 429, 1169, 322, 1784, 5195, 1806, 29879, 322, 2919, 17266, 403, 17759, 414, 526, 6819, 1009, 29199, 1090, 278, 3593, 29889, 13, 2855, 565, 21544, 508, 367, 6206, 322, 278, 14582, 393, 505, 14792, 508, 7910, 1449, 6133, 25227, 355, 17498, 363, 4808, 414, 310, 6133, 29899, 29891, 969, 292, 3987, 29892, 1244, 338, 920, 304, 1708, 278, 16611, 593, 595, 29901, 960, 366, 6554, 596, 521, 2925, 310, 25227, 355, 16415, 6460, 368, 29892, 19417, 470, 15649, 3099, 297, 18032, 545, 448, 274, 10003, 701, 278, 25227, 355, 7709, 565, 372, 338, 4482, 29892, 322, 788, 26438, 29892, 20118, 880, 470, 26343, 313, 465, 9929, 372, 674, 6773, 304, 14451, 297, 8666, 29897, 565, 2712, 297, 17182, 4845, 457, 29889, 13, 4205, 25071, 29901, 306, 626, 29914, 705, 526, 1472, 478, 1254, 29931, 29889, 13, 29902, 5456, 445, 4274, 29889, 13, 13, 2277, 29937, 13291, 29901, 13, 23523, 13, 2277, 29937, 2796, 13, 268]\n",
      "1253\n",
      "[13291, 29901, 13, 23523, 13, 2277, 29937, 2796, 13, 268]\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = tokenizer(dataset[\"train\"][1][\"text\"], max_length=4096, truncation=True)\n",
    "print(sample_tokens['input_ids'])\n",
    "print(len(sample_tokens['input_ids']))\n",
    "print(sample_tokens['input_ids'][-10:])  # corresponds to \\nmachine not \\n machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 835], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [1, 835, 2796], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [1, 29871, 13, 2277, 29937, 2796], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [1, 29871, 13, 2277, 29937, 2796, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [1, 29871, 13, 23523, 13, 2277, 29937, 2796, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [1, 4933], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [1, 29871, 13, 23523], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [1, 29871, 13, 26029, 13, 2277, 29937, 2796, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [1, 5199], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [1, 29871, 13, 4933, 13, 2277, 29937, 2796, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"###\"))\n",
    "print(tokenizer(\"### End\"))\n",
    "print(tokenizer(\"\\n### End\"))\n",
    "print(tokenizer(\"\\n### End\\n\"))\n",
    "print(tokenizer(\"\\nmachine\\n### End\\n\"))\n",
    "print(tokenizer(\"machine\"))\n",
    "print(tokenizer(\"\\nmachine\"))\n",
    "print(tokenizer(\"\\nhuman\\n### End\\n\"))\n",
    "print(tokenizer(\"human\"))\n",
    "print(tokenizer(\"\\n machine\\n### End\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sooo, we don't get tokenizer upto 4096? the rest will be padded during training, perhaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa1be2708de4ceb925baad2b6075535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the entire dataset\n",
    "MAX_TOKENS = 4096\n",
    "_preproc_func = partial(tokenize_text, tokenizer=tokenizer, max_length = MAX_TOKENS)\n",
    "dataset[\"train\"] = dataset[\"train\"].map(_preproc_func)\n",
    "\n",
    "# takes a while... ~1-1.5 min for dataset[\"train\"]\n",
    "# same if run with batched=True (batching does happen though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
